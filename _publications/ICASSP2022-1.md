---
title: "MULTI-MODAL ACOUSTIC-ARTICULATORY FEATURE FUSION FOR DYSARTHRIC SPEECH RECOGNITION."
collection: publications
permalink: /publication/ICASSP2022-1
excerpt: 
date: 2022--05
venue: 'ICASSP'
paperurl: 
citation: 'Yue. Zhengjun, Loweimi. Erfan, Cvetkovic. Zoran, Christensen. Heidi, and Barker, Jon. (2020).
&quot;MULTI-MODAL ACOUSTIC-ARTICULATORY FEATURE FUSION FOR DYSARTHRIC SPEECH RECOGNITION.&quot;
<i>ICASSP 2022</i>, pages '
---
There has been much recent interest in building continuous speech recognition systems for people with severe speech impairments, e.g., dysarthria. However, the datasets that are commonly used are typically designed for tasks other than ASR development, or they contain only isolated words. As such, they contain much overlap in the prompts read by the speakers. Previous ASR evaluations have often neglected this, using language models (LMs) trained on non-disjoint training and test data, potentially producing unrealistically optimistic results. In this paper, we investigate the impact of LM design using the widely used TORGO database. We combine state-of-the-art acoustic models with LMs trained with data originating from LibriSpeech. Using LMs with varying vocabulary size, we examine the trade-off between the out-of-vocabulary rate and recognition confusions for speakers with varying degrees of dysarthria. It is found that the optimal LM complexity is highly speaker dependent, highlighting the need to design speaker-dependent LMs alongside speaker-dependent acoustic models when considering atypical speech.

[Download paper here]()

[//]: # (Recommended citation: Yue. Zhengjun, Loweimi. Erfan, Cvetkovic. Zoran, Christensen. Heidi, and Barker, Jon. &#40;2022&#41;.)

[//]: # ("MULTI-MODAL ACOUSTIC-ARTICULATORY FEATURE FUSION FOR DYSARTHRIC SPEECH RECOGNITION.")

[//]: # (<i>ICASSP 2022</i>.)
